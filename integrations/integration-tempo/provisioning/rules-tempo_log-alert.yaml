apiVersion: grizzly.grafana.com/v1alpha1
kind: AlertRuleGroup
metadata:
    name: Integration-Tempo.log-alerts
spec:
    folderUid: integration-tempo
    interval: 60
    rules:
        - annotations:
            description: 'Tempo dropped traces in cluster {{ $labels.cluster }}, namespace {{ $labels.namespace }} due to size limits (e.g., >524KB). Review trace volume or increase limits.'
            summary: 'Tempo trace too large detected in {{ $labels.cluster }}/{{ $labels.namespace }}'
          condition: C
          data:
            - datasourceUid: duplo-logging
              model:
                datasource:
                    type: loki
                    uid: duplo-logging
                editorMode: code
                expr: sum(count_over_time({service_name="tempo"} | json | logfmt | drop __error__, __error_details__ | caller=~".*rate_limited_logger.*" | msg="TRACE_TOO_LARGE" [$__auto])) by (msg, cluster, namespace, pod)
                hide: false
                instant: true
                intervalMs: 1000
                maxDataPoints: 43200
                queryType: instant
                refId: A
              queryType: instant
              refId: A
              relativeTimeRange:
                from: 600
            - datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - C
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: A
                intervalMs: 1000
                maxDataPoints: 43200
                refId: C
                type: threshold
              refId: C
              relativeTimeRange: {}
          execErrState: Error
          folderUID: integration-tempo
          for: 1m0s
          labels:
            severity: warning
            owner: duplo
          noDataState: OK
          orgID: 1
          ruleGroup: log-alerts
          title: Tempo rate limit error
          uid: Temporatelimiterror
        - annotations:
            description: |-
                The compactor in Tempo (cluster: {{ $labels.cluster }}, namespace: {{ $labels.namespace }}) was marked suspect, possibly due to network issues or failure to communicate with other ring members.
            summary: 'Tempo compactor {{ $labels.cluster }}/{{ $labels.namespace }} is unstable'
          condition: C
          data:
            - datasourceUid: duplo-logging
              model:
                datasource:
                    type: loki
                    uid: duplo-logging
                editorMode: code
                expr: sum(count_over_time({service_name="tempo"} |~ "(?i)(Marking duplo-tracing-compactor|Suspect duplo-tracing-compactor)" | json | logfmt | drop __error__, __error_details__ | caller=~".*memberlist_logger.go.*"  [$__auto])) by (msg, cluster, namespace, pod)
                hide: false
                instant: true
                intervalMs: 1000
                maxDataPoints: 43200
                queryType: instant
                refId: A
              queryType: instant
              refId: A
              relativeTimeRange:
                from: 1800
            - datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - C
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: A
                intervalMs: 1000
                maxDataPoints: 43200
                refId: C
                type: threshold
              refId: C
              relativeTimeRange: {}
          execErrState: Error
          folderUID: integration-tempo
          for: 1m0s
          labels:
            severity: critical
            owner: duplo
          noDataState: OK
          orgID: 1
          ruleGroup: log-alerts
          title: Tempo Compactor Failure Alert
          uid: Tempocompactorfailurealert
        - annotations:
            description: |-
                More than 5 `WriteTo failed` warnings were detected in Tempo ingester pod `{{ $labels.pod }}` within namespace `{{ $labels.namespace }}` on cluster `{{ $labels.cluster }}` over the last 5 minutes.
                These errors typically indicate memberlist (gossip protocol) communication issues, which may impact cluster stability or ingestion performance.
                Investigate underlying network issues or resource pressure on the affected pod.
            summary: Tempo Ingester Memberlist WriteTo Failures Detected in {{ $labels.cluster }}/{{ $labels.namespace }}/{{ $labels.pod }}
          condition: C
          data:
            - datasourceUid: duplo-logging
              model:
                datasource:
                    type: loki
                    uid: duplo-logging
                editorMode: code
                expr: sum by (cluster, namespace, pod) (count_over_time({pod=~"duplo-tracing-ingester-.*"} |= "WriteTo failed"[5m]))
                instant: true
                intervalMs: 1000
                maxDataPoints: 43200
                queryType: instant
                refId: A
              queryType: instant
              refId: A
              relativeTimeRange:
                from: 600
            - datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 5
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - C
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: A
                intervalMs: 1000
                maxDataPoints: 43200
                refId: C
                type: threshold
              refId: C
              relativeTimeRange: {}
          execErrState: Error
          folderUID: integration-tempo
          for: 2m0s
          labels:
            severity: warning
            owner: duplo
          noDataState: OK
          orgID: 1
          ruleGroup: log-alerts
          title: Tempo-ingester-warning
          uid: Tempoingesterwarning
        - annotations:
            description: Detected an "Exporting failed." error log level=error from pod {{ $labels.pod }}, namespace {{ $labels.namespace }}, cluster {{ $labels.cluster }} in the last 5 minutes.
            summary: 'Exporting failed error in alloy service on pod {{ $labels.pod }} (namespace: {{ $labels.namespace }}, cluster: {{ $labels.cluster }})'
          condition: C
          data:
            - datasourceUid: duplo-logging
              model:
                datasource:
                    type: loki
                    uid: duplo-logging
                editorMode: code
                expr: |-
                    sum by (cluster, namespace, pod) (
                      count_over_time({service_name="alloy"} |= "Exporting failed." | logfmt | level="error"[5m])
                    )
                instant: true
                intervalMs: 1000
                maxDataPoints: 43200
                queryType: range
                refId: A
              queryType: range
              refId: A
              relativeTimeRange:
                from: 600
            - datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 5
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - C
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: A
                intervalMs: 1000
                maxDataPoints: 43200
                refId: C
                type: threshold
              refId: C
              relativeTimeRange: {}
          execErrState: OK
          folderUID: integration-tempo
          for: 5m0s
          labels:
            owner: duplo
            severity: critical
          noDataState: NoData
          orgID: 1
          ruleGroup: log-alerts
          title: AlloyExportingFailed
          uid: Alloyexportingfailed
    title: log-alerts