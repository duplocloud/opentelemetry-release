apiVersion: grizzly.grafana.com/v1alpha1
kind: PrometheusRuleGroup
metadata:
    name: gossip_alerts
    namespace: integration-mimir
spec:
    rules:
        - alerts: []
          annotations:
            message: One or more Mimir instances in {{ $labels.cluster }}/{{ $labels.namespace }} consistently sees a higher than expected number of gossip members.
            runbook_url: https://grafana.com/docs/mimir/latest/operators-guide/mimir-runbooks/#mimirgossipmemberstoohigh
          duration: 1200
          labels:
            severity: warning
          name: MimirGossipMembersTooHigh
          query: max by (cluster, namespace) (memberlist_client_cluster_members_count{cluster="duplo-metrics"}) > (sum by (cluster, namespace) (up{job=~".+/(admin-api|alertmanager|compactor.*|distributor.*|ingester.*|querier.*|ruler|ruler-querier.*|store-gateway.*|cortex|mimir|mimir-write.*|mimir-read.*|mimir-backend.*)"}) + 10)
          type: alerting
        - alerts:
            - annotations:
                message: One or more Mimir instances in duploinfra-o11y/duploservices-otel-o11y consistently sees a lower than expected number of gossip members.
                runbook_url: https://grafana.com/docs/mimir/latest/operators-guide/mimir-runbooks/#mimirgossipmemberstoolow
              labels:
                alertname: MimirGossipMembersTooLow
                cluster: duploinfra-o11y
                namespace: duploservices-otel-o11y
                severity: warning
              value: "1e+00"
          annotations:
            message: One or more Mimir instances in {{ $labels.cluster }}/{{ $labels.namespace }} consistently sees a lower than expected number of gossip members.
            runbook_url: https://grafana.com/docs/mimir/latest/operators-guide/mimir-runbooks/#mimirgossipmemberstoolow
          duration: 1200
          labels:
            severity: warning
          name: MimirGossipMembersTooLow
          query: min by (cluster, namespace) (memberlist_client_cluster_members_count{cluster="duplo-metrics"}) < (sum by (cluster, namespace) (up{job=~".+/(admin-api|alertmanager|compactor.*|distributor.*|ingester.*|querier.*|ruler|ruler-querier.*|store-gateway.*|cortex|mimir|mimir-write.*|mimir-read.*|mimir-backend.*)"}) * 0.5)
          type: alerting
        - alerts: []
          annotations:
            message: Mimir gossip-ring service endpoints list in {{ $labels.cluster }}/{{ $labels.namespace }} is out of sync.
            runbook_url: https://grafana.com/docs/mimir/latest/operators-guide/mimir-runbooks/#mimirgossipmembersendpointsoutofsync
          duration: 900
          labels:
            severity: warning
          name: MimirGossipMembersEndpointsOutOfSync
          query: (count by (cluster, namespace) (kube_endpoint_address{endpoint="gossip-ring"} unless on (cluster, namespace, ip) label_replace(kube_pod_info, "ip", "$1", "pod_ip", "(.*)")) / count by (cluster, namespace) (kube_endpoint_address{endpoint="gossip-ring"}) * 100 > 10) and (count by (cluster, namespace) (cortex_build_info) > 0)
          type: alerting
        - alerts: []
          annotations:
            message: Mimir gossip-ring service endpoints list in {{ $labels.cluster }}/{{ $labels.namespace }} is out of sync.
            runbook_url: https://grafana.com/docs/mimir/latest/operators-guide/mimir-runbooks/#mimirgossipmembersendpointsoutofsync
          duration: 300
          labels:
            severity: critical
          name: MimirGossipMembersEndpointsOutOfSync
          query: (count by (cluster, namespace) (kube_endpoint_address{endpoint="gossip-ring"} unless on (cluster, namespace, ip) label_replace(kube_pod_info, "ip", "$1", "pod_ip", "(.*)")) / count by (cluster, namespace) (kube_endpoint_address{endpoint="gossip-ring"}) * 100 > 50) and (count by (cluster, namespace) (cortex_build_info) > 0)
          type: alerting
