apiVersion: grizzly.grafana.com/v1alpha1
kind: PrometheusRuleGroup
metadata:
    name: node-exporter-alerts
    namespace: integration-linuxnode
spec:
    rules:
        - alerts: []
          annotations:
            description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} receive errors in the last two minutes.'
            summary: Network interface is reporting many receive errors.
          duration: 3600
          labels:
            severity: warning
          name: NodeNetworkReceiveErrs
          query: rate(node_network_receive_errs_total{job=~"integrations/(node_exporter|unix)"}[2m]) / rate(node_network_receive_packets_total{job=~"integrations/(node_exporter|unix)"}[2m]) > 0.01
          type: alerting
        - alerts: []
          annotations:
            description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
            summary: Network interface is reporting many transmit errors.
          duration: 3600
          labels:
            severity: warning
          name: NodeNetworkTransmitErrs
          query: rate(node_network_transmit_errs_total{job=~"integrations/(node_exporter|unix)"}[2m]) / rate(node_network_transmit_packets_total{job=~"integrations/(node_exporter|unix)"}[2m]) > 0.01
          type: alerting
        - alerts: []
          annotations:
            description: '{{ $value | humanizePercentage }} of conntrack entries are used.'
            summary: Number of conntrack are getting close to the limit.
          duration: 0
          labels:
            severity: warning
          name: NodeHighNumberConntrackEntriesUsed
          query: (node_nf_conntrack_entries{job=~"integrations/(node_exporter|unix)"} / node_nf_conntrack_entries_limit) > 0.75
          type: alerting
        - alerts: []
          annotations:
            description: Node Exporter text file collector on {{ $labels.instance }} failed to scrape.
            summary: Node Exporter text file collector failed to scrape.
          duration: 0
          labels:
            severity: warning
          name: NodeTextFileCollectorScrapeError
          query: node_textfile_scrape_error{job=~"integrations/(node_exporter|unix)"} == 1
          type: alerting
        - alerts: []
          annotations:
            description: Clock at {{ $labels.instance }} is out of sync by more than 0.05s. Ensure NTP is configured correctly on this host.
            summary: Clock skew detected.
          duration: 600
          labels:
            severity: warning
          name: NodeClockSkewDetected
          query: (node_timex_offset_seconds{job=~"integrations/(node_exporter|unix)"} > 0.05 and deriv(node_timex_offset_seconds{job=~"integrations/(node_exporter|unix)"}[5m]) >= 0) or (node_timex_offset_seconds{job=~"integrations/(node_exporter|unix)"} < -0.05 and deriv(node_timex_offset_seconds{job=~"integrations/(node_exporter|unix)"}[5m]) <= 0)
          type: alerting
        - alerts: []
          annotations:
            description: Clock at {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.
            summary: Clock not synchronising.
          duration: 600
          labels:
            severity: warning
          name: NodeClockNotSynchronising
          query: min_over_time(node_timex_sync_status{job=~"integrations/(node_exporter|unix)"}[5m]) == 0 and node_timex_maxerror_seconds{job=~"integrations/(node_exporter|unix)"} >= 16
          type: alerting
        - alerts: []
          annotations:
            summary: RAID Array is degraded.
          duration: 900
          labels:
            severity: critical
          name: NodeRAIDDegraded
          query: node_md_disks_required{device!="",job=~"integrations/(node_exporter|unix)"} - ignoring (state) (node_md_disks{device!="",job=~"integrations/(node_exporter|unix)",state="active"}) > 0
          type: alerting
        - alerts: []
          annotations:
            description: At least one device in RAID array at {{ $labels.instance }} failed. Array '{{ $labels.device }}' needs attention and possibly a disk swap.
            summary: Failed device in RAID array.
          duration: 0
          labels:
            severity: warning
          name: NodeRAIDDiskFailure
          query: node_md_disks{device!="",job=~"integrations/(node_exporter|unix)",state="failed"} > 0
          type: alerting
        - alerts: []
          annotations:
            description: File descriptors limit at {{ $labels.instance }} is currently at {{ printf "%.2f" $value }}%.
            summary: Kernel is predicted to exhaust file descriptors limit soon.
          duration: 900
          labels:
            severity: warning
          name: NodeFileDescriptorLimit
          query: (node_filefd_allocated{job=~"integrations/(node_exporter|unix)"} * 100 / node_filefd_maximum{job=~"integrations/(node_exporter|unix)"} > 70)
          type: alerting
        - alerts: []
          annotations:
            description: File descriptors limit at {{ $labels.instance }} is currently at {{ printf "%.2f" $value }}%.
            summary: Kernel is predicted to exhaust file descriptors limit soon.
          duration: 900
          labels:
            severity: critical
          name: NodeFileDescriptorLimit
          query: (node_filefd_allocated{job=~"integrations/(node_exporter|unix)"} * 100 / node_filefd_maximum{job=~"integrations/(node_exporter|unix)"} > 90)
          type: alerting
        - alerts: []
          annotations:
            description: |
                CPU usage at {{ $labels.instance }} has been above 90% for the last 15 minutes, is currently at {{ printf "%.2f" $value }}%.
            summary: High CPU usage.
          duration: 900
          labels:
            severity: info
          name: NodeCPUHighUsage
          query: sum without (mode) (avg without (cpu) (rate(node_cpu_seconds_total{job=~"integrations/(node_exporter|unix)",mode!="idle"}[2m]))) * 100 > 90
          type: alerting
        - alerts: []
          annotations:
            description: |
                System load per core at {{ $labels.instance }} has been above 2 for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
                This might indicate this instance resources saturation and can cause it becoming unresponsive.
            summary: System saturated, load per core is very high.
          duration: 900
          labels:
            severity: warning
          name: NodeSystemSaturation
          query: node_load1{job=~"integrations/(node_exporter|unix)"} / count without (cpu, mode) (node_cpu_seconds_total{job=~"integrations/(node_exporter|unix)",mode="idle"}) > 2
          type: alerting
        - alerts: []
          annotations:
            description: |
                Memory major pages are occurring at very high rate at {{ $labels.instance }}, 500 major page faults per second for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
                Please check that there is enough memory available at this instance.
            summary: Memory major page faults are occurring at very high rate.
          duration: 900
          labels:
            severity: warning
          name: NodeMemoryMajorPagesFaults
          query: rate(node_vmstat_pgmajfault{job=~"integrations/(node_exporter|unix)"}[5m]) > 500
          type: alerting
        - alerts: []
          annotations:
            description: |
                Memory is filling up at {{ $labels.instance }}, has been above 90% for the last 15 minutes, is currently at {{ printf "%.2f" $value }}%.
            summary: Host is running out of memory.
          duration: 900
          labels:
            severity: warning
          name: NodeMemoryHighUtilization
          query: 100 - (node_memory_MemAvailable_bytes{job=~"integrations/(node_exporter|unix)"} / node_memory_MemTotal_bytes{job=~"integrations/(node_exporter|unix)"} * 100) > 90
          type: alerting
        - alerts: []
          annotations:
            description: |
                Disk IO queue (aqu-sq) is high on {{ $labels.device }} at {{ $labels.instance }}, has been above 10 for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
                This symptom might indicate disk saturation.
            summary: Disk IO queue is high.
          duration: 1800
          labels:
            severity: warning
          name: NodeDiskIOSaturation
          query: rate(node_disk_io_time_weighted_seconds_total{device!="",job=~"integrations/(node_exporter|unix)"}[5m]) > 10
          type: alerting
        - alerts: []
          annotations:
          duration: 300
          labels:
            severity: warning
          name: NodeSystemdServiceFailed
          query: node_systemd_unit_state{job=~"integrations/(node_exporter|unix)",state="failed"} == 1
          type: alerting
        - alerts: []
          annotations:
            description: Systemd service {{ $labels.name }} has been restarted too many times at {{ $labels.instance }} for the last 15 minutes. Please check if service is crash looping.
            summary: Systemd service keeps restaring, possibly crash looping.
          duration: 900
          labels:
            severity: warning
          name: NodeSystemdServiceCrashlooping
          query: increase(node_systemd_service_restart_total{job=~"integrations/(node_exporter|unix)"}[5m]) > 2
          type: alerting
